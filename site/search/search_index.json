{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"akhir/","text":"Hasil akhir dari program akan menunjukkan data-data yang memiliki kemiripan atau karakteristik yang sama di masing-masing cluter. Untuk mendapatkan sebuah cluster dengan karakteristik yang lebih mirip, bisa dengan menambahkan jumlah cluster saat proses clustering. Seleksi fitur sangat berpengaruh besar pada hasil cluster, fitur yang sangat banyak dapat dikurangi / dilakukan penyusutan dengan metode pearson correlation. pearson correlation adalah metode yang paling sederhana untuk seleksi fitur, sehingga hasil akurasi masih kurang akurat. untuk itu disarankan untuk mencoba melakukan perbandingan akurasi dengan metode lain untuk mencari hasil yang lebih akurat.","title":"Kesimpulan"},{"location":"clustering/","text":"Clustering dilakukan untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018class\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018class\u2019 yang lain. Pada permasalahan ini digunakan fuzzy c-means, untuk mengelopokkan data menjadi 3 class. cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) parameter dari fuzzy c-means : data, jumlah cluster, pembobot, eror maksimal dan iterasi maksimal. Menghitung Silhouette untuk mengetahui jarak kedekatan atau realasi fitur dalam tiap cluster silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) Simpan hasil clustering ke file csv def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) write_csv(\"Cluster.csv\", [[\"Cluster\"]]) write_csv(\"Cluster.csv\", [membership], \"a\") write_csv(\"Cluster.csv\", [[\"silhouette\"]], \"a\") write_csv(\"Cluster.csv\", [silhouette], \"a\") write_csv(\"Cluster.csv\", [[\"Keanggotaan\"]], \"a\") write_csv(\"Cluster.csv\", u, \"a\") write_csv(\"Cluster.csv\", [[\"pusat Cluster\"]], \"a\") write_csv(\"Cluster.csv\", cntr, \"a\")","title":"Clustering"},{"location":"crawling/","text":"Lakukan import untuk requirement : from requests import get from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from math import log10 import numpy as np from sklearn.metrics import silhouette_samples, silhouette_score import skfuzzy as fuzz Proses pertama adalah melakukan scan/crawl pada halaman web, dan mengambil data yang diinginkan. Proses crawl : judul=[] developer=[] deskripsi=[] urls=[str(i) for i in range (1,11)] Buat list untuk menampung masing-masing data yang kita inginkan, yaitu data judul, developer dan deskripsi dari 10 halaman. List urls digunakan untuk menampung page/halaman yang diinginkan, dengan range dari page ke-1 sampai page ke-10. pada program kita set range 1-11 karena stop looping dari program adalah n-1 for url in urls : page=get(\"https://www.dicoding.com/events?q=&criteria=&sort=&sort_direction=desc&page=\"+url) Lakukan get pada alamat website yang akan dicrawl, dapat dilihat pada bagian belakang alamat website terdapat variable url . url menampung informasi nomor halaman yang akan dicrawl. contoh : urls = 1 sampai 10 kita mulai dari, url = 1 page=get(\"https://www.dicoding.com/events?q=&criteria=&sort=&sort_direction=desc&page=\"+\"1\") link : https://www.dicoding.com/events?q=&criteria=&sort=&sort_direction=desc&page=1 maka akan mengakses page ke-1 dari website Iterasi akan terus berjalan sampai url=10 soup=BeautifulSoup(page.content,'html.parser') jdl=soup.findAll(class_='item-box-name') dev=soup.findAll('div',attrs={'class':'item-box-main-information'}) desk=soup.findAll('div',attrs={'class':'item-box-main-information'}) soup.findAll() digunakan untuk mencari dan mengambil data berdasarkan class pada tag html dari website. Untuk mendapatkan class tag html perlu dilakukan inspeksi elemen. cara melakukan inspeksi elemen : block informasi yang dicari - klik kanan - inspeksi elemen - cari class yang menampung informasi data judul : <a class='item-box-name'> data developer : <div class='item-box-main-information'> data deskripsi : <div class='item-box-main-information'> for a in range (len(jdl)): judul+=[jdl[a].getText()] for b in range (len(dev)): developer+=[dev[b].find('p').text] for c in range(len(desk)): deskripsi+=[desk[c].find_all('p')[1].text] .getText() dan .text digunakan untuk mengubah data menjadi text yang kemudian disimpan pada list yang sudah dibuat sebelumnya. Save data ke db : Setelah berhasil mengambil data dari setiap halaman, kita tampung dalam db menggunakan sqlite. conn = sqlite3.connect('events.db') conn.execute('''CREATE TABLE if not exists EVENTS (NAMA_EVENT VARCHAR NOT NULL, DEVELOPER VARCHAR NOT NULL, DESKRIPSI VARCHAR NOT NULL);''') for i in range (len(judul)): conn.execute('INSERT INTO EVENTS(NAMA_EVENT,DEVELOPER,DESKRIPSI) values (?, ?, ?)', (judul[i], developer[i], deskripsi[i])) Code diatas digunakan untuk membuat db events , dengan nama tabel EVENTS dan 3 kolom yaitu NAMA_EVENT , DEVELOPER dan DESKRIPSI. Kemudian insert setiap data yang terdapat pada list judul , developer dan deskripsi ke setiap kolom pada tabel.","title":"Crawling"},{"location":"fitur/","text":"Proses seleksi fitur digunakan untuk mengurangi jumlah kata/fitur yang tidak penting (sesuai hasil pembobotan), banyaknya fitur sangat berpengaruh pada hasil akhir clustering dan waktu untuk komputasi oleh karena itu seleksi fitur sangat dibutuhkan untuk melakukan penyusutan pada fitur yang akan digunakan. Pearson Correlation Merupakan metode seleksi fitur sederhana dengan cara mengukur korelasi/hubungan setiap fitur, semakin tinggi nilai korelasi maka semakin kuat hubungan fiturnya. def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return (atas/(bawah_kiri * bawah_kanan)) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil)","title":"Seleksi Fitur"},{"location":"home/","text":"Web Mining 2019 / Weni Pratiwi S (160411100013) Penambangan dan pencarian web atau web mining merupakan proses ektraksi pola dari data-data pada suatu website. Terdiri dari 3 bagian yaitu : Web content mining Web content mining adalah proses ekstraksi pola/informasi dari dokumen atau data. Cara kerjanya adalah dengan cara mengekstraksi key word dari data, bisa berupa teks, citra, audio, video, metadata dan hyperlink. Web structure mining Web structure mining (log mining) digunakan untuk menemukan struktur link dari suatu website, yang nantinya akan membentuk suatu rangkuman dari bangunan website. Web usage mining Web usage mining adalah proses pengambilan informasi perilaku user dari log, click stream, query dan cookies. Untuk mendapatkan informasi dari web kita perlu melakukan crawl yaitu membaca semua halaman dan membuat index dari data yang dicari, dengam menggunakan Web Crawler . Web crawler merupakan serangkaian script pemrograman yang digunakan untuk melakukan crawl secara otomatis, dan menyimpan data yang diperoleh untuk selanjutnya diolah untuk mendapatkan informasi yang diinginkan. Requirement : Python BeatifulSoup4 SQLite3 Numpy Scipy Sastrawi Sckit-learn Sckit-fuzzy Math Database KBI import CSV Website : \"https://www.dicoding.com/events?q=&criteria=&sort=&sort_direction=desc&page=1\" Install requirement menggunakan pip di command prompt, atau download Anaconda. Library Anaconda sudah lengkap, anda bisa langsung menggunakan library yang anda butuhkan untuk bahasa python.","title":"Pendahuluan"},{"location":"home/#requirement","text":"Python BeatifulSoup4 SQLite3 Numpy Scipy Sastrawi Sckit-learn Sckit-fuzzy Math Database KBI import CSV Website : \"https://www.dicoding.com/events?q=&criteria=&sort=&sort_direction=desc&page=1\" Install requirement menggunakan pip di command prompt, atau download Anaconda. Library Anaconda sudah lengkap, anda bisa langsung menggunakan library yang anda butuhkan untuk bahasa python.","title":"Requirement :"},{"location":"prepro/","text":"Pada tahap ini, data yang telah kita dapat perlu dilakukan pre-processing atau seleksi untuk mendapatkan key word yang akan menjadi sebuah fitur (kata penting). Tahapan preprocessing : Tokenisasi, proses pemecahan kata menjadi index-index tersendiri Stopword, proses seleksi membuang kata yang termasuk kata hubung dan tanda baca Stemming, proses pengambilan kata dasar dari setiap kata Seleksi berdasarkan KBI, kata dasar yang telah didapat dari porses stemming akan dicocokan pada kata yang terdapat pada database KBI untuk mendapatkan kata baku saja. Menambahkan kosa kata tidak penting pada list stopword dengan cara manual, yaitu mengakses Sastrawi\\StopWordRemover\\StopWordRemoverFactory.py dari library python anda Anaconda3\\Lib\\site-packages\\ . ekstraksi kata dasar : factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover () factorym = StemmerFactory () stemmer = factorym.create_stemmer () tmp = '' for i in deskripsi: tmp = tmp + ' ' +i hasil = [] for i in tmp.split(): try : if i.isalpha() and (not i in hasil) and len(i)>2: # Menghilangkan Kata tidak penting stop = stopword.remove(i) if stop != \"\": out = stemmer.stem(stop) hasil.append(out) except: continue katadasar=hasil Code diatas digunakan untuk mengekstaksi kata dasar, membuang fitur yang bukan berupa text dan fitur yang termasuk kata hubung menggunakan library dari sastrawi. contoh : input : \"saya jalan2 membeli sayur di pasar\" output : \"saya\" \"jalan\" \"beli\" \"sayur\" \"pasar\" cek kata dalam KBI : conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found Dalam proses ini kita memerlukan database KBI untuk mengecek setiap kata yang sesuai/baku. Kemudian buat sebuah function LinearSearch untuk mengecek setiap kata pada katadasar yang sesuai dengan KBI, dengan return value found . found bernilai true - false berhasil=[] for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) Jika found=true maka kata tersebut merupakan kata baku sesuai KBI, jadi kata tersebut kita simpan pada list berhasil . Jika found=false maka kata tersebut bukan kata baku sesuai KBI, jadi kata tersebut tidak perlu disimpan.","title":"Preprocessing"},{"location":"tfidf/","text":"TF-IDF atau Term Frequence dan Invers Document Frequence adalah proses pembobotan term pada setiap data yang heterogen. menghitung TF TF sama dengan VSM yaitu menghitung frekuensi kemunculan kata pada setiap data. tf = matrix menghitung IDF menghitung jumlah data secara luas pada koleksi dokumen yang bersangkutan. menghitung DF (Document Frekuensi) contoh : data 1 = \"telur ayam goreng\" data 2 = \"masak telur goreng makan ayam goreng\" kata jumlah data yang mengandung kata tersebut ayam 2 goreng 2 masak 1 makan 1 telur 2 df = list() for d in range (len(matrix[0])): total = 0 for i in range(len(matrix)): if matrix[i][d] !=0: total += 1 df.append(total) idf = list() for i in df: tmp = 1 + log10(len(matrix)/(1+i)) idf.append(tmp) list df digunakan untuk menampung hasil perhitungan DF, kemudian kita hitung IDF yang merupakan hasil inverse dari DF dan kita tampung pada list idf . tfidf = [] for baris in range(len(matrix)): tampungBaris = [] for kolom in range(len(matrix[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) Hitung TF-IDF, dengan mengalikan TF dengan IDF tf-idf = tf x idf , kemudian hasil tfidf disimpan pada list tfidf . with open('TFIDF.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(berhasil) for i in tfidf: employee_writer.writerow(i) tampung hasil TF-IDF pada file csv. TF-IDF digunakan untuk mendapatkan bobot yang lebih akurat dibandingkan dengan hasil VSM.","title":"TF-IDF"},{"location":"vsm/","text":"VSM merupakan proses menghitung frekuensi kemunculan setiap kata pada setiap data, dalam bentuk matriks. contoh : data 1 = \"telur ayam goreng\" data 2 = \"masak telur goreng makan ayam goreng\" data ke ayam goreng masak makan telur 1 1 1 0 0 1 2 1 2 1 1 1 conn = sqlite3.connect('events.db') matrix=[] cursor = conn.execute(\"SELECT* from EVENTS\") for row in cursor: tampung = [] for i in berhasil: tampung.append(row[2].lower().count(i)) matrix.append(tampung) code diatas akan mengecek dan menghitung berapa kali kata pada list berhasil muncul pada setiap baris data dalam tabel EVENTS di database events.db , kemudian menampung hasil frekuensi kemunculan .count() dalam list matrix . with open('VSMkbi.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(berhasil) for i in matrix: employee_writer.writerow(i) kemudian simpan hasil dalam file csv, dengan kolom pertama berisi kata/fitur dan kolom selanjutnya berisi matrix frekuensi kemunculan.","title":"VSM"}]}