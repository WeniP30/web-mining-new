{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"akhir/","text":"Hasil akhir dari program akan menunjukkan data-data yang memiliki kemiripan atau karakteristik yang sama di masing-masing cluter. Untuk mendapatkan sebuah cluster dengan karakteristik yang lebih mirip, bisa dengan menambahkan jumlah cluster saat proses clustering. Seleksi fitur sangat berpengaruh besar pada hasil cluster, fitur yang sangat banyak dapat dikurangi / dilakukan penyusutan dengan metode pearson correlation. pearson correlation adalah metode yang paling sederhana untuk seleksi fitur, sehingga hasil akurasi masih kurang akurat. untuk itu disarankan untuk mencoba melakukan perbandingan akurasi dengan metode lain untuk mencari hasil yang lebih akurat.","title":"Kesimpulan"},{"location":"clustering/","text":"Clustering dilakukan untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018class\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018class\u2019 yang lain. Pada permasalahan ini digunakan fuzzy c-means, untuk mengelopokkan data menjadi 3 class. cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) parameter dari fuzzy c-means : data, jumlah cluster, pembobot, eror maksimal dan iterasi maksimal. Menghitung Silhouette untuk mengetahui jarak kedekatan atau realasi fitur dalam tiap cluster silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) Simpan hasil clustering ke file csv def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) write_csv(\"Cluster.csv\", [[\"Cluster\"]]) write_csv(\"Cluster.csv\", [membership], \"a\") write_csv(\"Cluster.csv\", [[\"silhouette\"]], \"a\") write_csv(\"Cluster.csv\", [silhouette], \"a\") write_csv(\"Cluster.csv\", [[\"Keanggotaan\"]], \"a\") write_csv(\"Cluster.csv\", u, \"a\") write_csv(\"Cluster.csv\", [[\"pusat Cluster\"]], \"a\") write_csv(\"Cluster.csv\", cntr, \"a\")","title":"Clustering"},{"location":"crawl_link/","text":"Kali ini kita akan membahas mengenai Web Structure Mining , sama seperti sebelumnya kita akan melakukan crawl website hanya saja kali ini kita akan mengambil hyperlink yang ada pada setiap page dari website. Hyperlink yang kita dapatkan nantinya digunakan untuk mengetahui pagerank dan struktur link dari website. Semakin tinggi pagerank maka semakin kuat pengaruh situs/page tersebut terhadap page lainnya. Struktur link yang dipatkan akan berbentuk graph berarah, yang terdiri dari node dan edge . Node merupakan situs/page dan Edge merupakan penghubung antar node yang melambangkan hubungan antara node itu sendiri. Contoh struktur link : Proses Crawl Link : Website : https://www.dicoding.com/ import pandas as pd import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt (1) Mengambil link pada html link biasanya ditampung di tag <a href='https://www.ex.com'> mylink</a> maka untuk mendapatkan seluruh link yang terdapat di website kita ambil semua isi 'href' pada tag <a> . def GETlink(src): try: page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') tags = soup.findAll(\"a\") links = [] for tag in tags: try: link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links except: return list() (2) Filter link link yang kita dapatkan perlu dilakukan filter untuk memastikan link tersebut merupakan link yang benar. def filterURL(url): #filter1 if \"www.\" in url: ind = url.index(\"www.\")+4 url = \"http://\"+url[ind:] #filter2 if url[-1] == \"/\": url = url[:-1] #filter3 parts = url.split(\"/\") url = '' for i in range(3): url += parts[i] + \"/\" return url filter 1 digunakan untuk menambahkan \"http://\" pada link dengan index awal \"www.\" filter 2 digunakan untuk mengecek \"/\" di akhir link atau link[-1]=\"/\", maka ambil link dari index link[-1] sampai index awal link atau link=link[:-1]. filter 3 digunakan untuk mengambil link utama, contoh : \"https://www.dicoding.com/academies/51\" maka kita hanya perlu ambil \"https://www.dicoding.com/\". (3) Crawl dua proses sebelumnya kita satukan dalam fungsi crawl berikut : def crawl(url, max_deep, show=False, deep=0, cek=[]): global edgelist deep += 1 url = filterURL(url) if not url in cek: links = GETlink(url) cek.append(url) if show: if deep == 1: print(url) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"-w\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: link = filterURL(link) edge = (url,link) if not edge in edgelist: edgelist.append(edge) if (deep != max_deep): crawl(link, max_deep, show, deep, cek) pada proses ini kita lakukan proses ambil link (GETlink(src)) dan filter link (filterURL(url)) dengan parameter url awal (url), maksimal kedalaman (max_deep), menampilkan proses crwal (show), kedalam awal (deep=0) dan list tempat menampung link yang sudah di ambil (cek=[]). if not url in cek digunakan untuk mengecek apakah link tersebut sudah pernah dicrawl atau tidak, jika pernah maka link tidak perlu dimasukkan pada edgelist , jika belum maka masukkan link pada edgelist . hasil dari proses crawl ini akan menghasilkan edgelist yang berisi link yang sudah difilter dan tanpa duplikasi link. Memanggil fungsi crawl : myurl = \"https://www.dicoding.com/\" nodelist = [myurl] edgelist = [] crawl(myurl, 3, show=True) edgelistBox = pd.DataFrame(edgelist, None, (\"From\", \"To\"))","title":"Crawl Link"},{"location":"crawling/","text":"Lakukan import untuk requirement : from requests import get from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from math import log10 import numpy as np from sklearn.metrics import silhouette_samples, silhouette_score import skfuzzy as fuzz Proses pertama adalah melakukan scan/crawl pada halaman web, dan mengambil data yang diinginkan. Proses crawl : judul=[] developer=[] deskripsi=[] urls=[str(i) for i in range (1,11)] Buat list untuk menampung masing-masing data yang kita inginkan, yaitu data judul, developer dan deskripsi dari 10 halaman. List urls digunakan untuk menampung page/halaman yang diinginkan, dengan range dari page ke-1 sampai page ke-10. pada program kita set range 1-11 karena stop looping dari program adalah n-1 for url in urls : page=get(\"https://www.dicoding.com/events?q=&criteria=&sort=&sort_direction=desc&page=\"+url) Lakukan get pada alamat website yang akan dicrawl, dapat dilihat pada bagian belakang alamat website terdapat variable url . url menampung informasi nomor halaman yang akan dicrawl. contoh : urls = 1 sampai 10 kita mulai dari, url = 1 page=get(\"https://www.dicoding.com/events?q=&criteria=&sort=&sort_direction=desc&page=\"+\"1\") link : https://www.dicoding.com/events?q=&criteria=&sort=&sort_direction=desc&page=1 maka akan mengakses page ke-1 dari website Iterasi akan terus berjalan sampai url=10 soup=BeautifulSoup(page.content,'html.parser') jdl=soup.findAll(class_='item-box-name') dev=soup.findAll('div',attrs={'class':'item-box-main-information'}) desk=soup.findAll('div',attrs={'class':'item-box-main-information'}) soup.findAll() digunakan untuk mencari dan mengambil data berdasarkan class pada tag html dari website. Untuk mendapatkan class tag html perlu dilakukan inspeksi elemen. cara melakukan inspeksi elemen : block informasi yang dicari - klik kanan - inspeksi elemen - cari class yang menampung informasi data judul : <a class='item-box-name'> data developer : <div class='item-box-main-information'> data deskripsi : <div class='item-box-main-information'> for a in range (len(jdl)): judul+=[jdl[a].getText()] for b in range (len(dev)): developer+=[dev[b].find('p').text] for c in range(len(desk)): deskripsi+=[desk[c].find_all('p')[1].text] .getText() dan .text digunakan untuk mengubah data menjadi text yang kemudian disimpan pada list yang sudah dibuat sebelumnya. Save data ke db : Setelah berhasil mengambil data dari setiap halaman, kita tampung dalam db menggunakan sqlite. conn = sqlite3.connect('events.db') conn.execute('''CREATE TABLE if not exists EVENTS (NAMA_EVENT VARCHAR NOT NULL, DEVELOPER VARCHAR NOT NULL, DESKRIPSI VARCHAR NOT NULL);''') for i in range (len(judul)): conn.execute('INSERT INTO EVENTS(NAMA_EVENT,DEVELOPER,DESKRIPSI) values (?, ?, ?)', (judul[i], developer[i], deskripsi[i])) Code diatas digunakan untuk membuat db events , dengan nama tabel EVENTS dan 3 kolom yaitu NAMA_EVENT , DEVELOPER dan DESKRIPSI. Kemudian insert setiap data yang terdapat pada list judul , developer dan deskripsi ke setiap kolom pada tabel.","title":"Crawling"},{"location":"fitur/","text":"Proses seleksi fitur digunakan untuk mengurangi jumlah kata/fitur yang tidak penting (sesuai hasil pembobotan), banyaknya fitur sangat berpengaruh pada hasil akhir clustering dan waktu untuk komputasi oleh karena itu seleksi fitur sangat dibutuhkan untuk melakukan penyusutan pada fitur yang akan digunakan. Pearson Correlation Merupakan metode seleksi fitur sederhana dengan cara mengukur korelasi/hubungan setiap fitur, semakin tinggi nilai korelasi maka semakin kuat hubungan fiturnya. def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return (atas/(bawah_kiri * bawah_kanan)) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil)","title":"Seleksi Fitur"},{"location":"graph/","text":"graph digunakan untuk visualisasi struktur link dari website. kita akan menggambar graph dari edgelist yang telah kita buat sebelumnya. g = nx.from_pandas_edgelist(edgelistBox, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show() kita gunakan Networkx untuk membentuk graph berarah, kemudian kita visualisasikan menggunakan Matplotlib . Berikut graph yang terbentuk dari website \"https://www.dicoding.com/\"","title":"Graph"},{"location":"pagerank/","text":"page rank merupakan nilai yang digunakan untuk mengukur link populer dari suatu website. page rank ditentukan oleh banyaknya link yang menuju ke website tersebut. biasanya digunakan untuk menentukan rangking sebuah website dalam mesin pencari. untuk menghitung page rank kita gunakan code berikut : damping = 0.85 max_loop = 100 error_toleransi = 0.0001 pr = nx.pagerank(g, alpha = damping, max_iter=max_loop, tol=error_toleransi) secara default damping factor bernilai 0.85 nodelist = g.nodes label= {} data = [] for i, key in enumerate(nodelist): data.append((pr[key], key)) label[key]=i page rank yang sudah kita hitung ditampung dalam list data, dengan key adalah node (link) dari website yang di crawl. kemudian urutkan page rank dari yang terbesar ke yang terkecil untuk menemukan page rank yang paling tinggi. page rank yang paling tinggi berarti node (link) sering diakses oleh node lainnya. urut = data.copy() for x in range(len(urut)): for y in range(len(urut)): if urut[x][0] > urut[y][0]: urut[x],urut[y] = urut[y],urut[x] urut = pd.DataFrame(urut, None, (\"PageRank\", \"Node\")) print(urut) Berikut hasil 10 page rank teratas dari \"https://www.dicoding.com/\" No Page rank Node 0 0.022732 http://google.com/ 1 0.020266 https://accounts.google.com/ 2 0.014959 https://twitter.com/ 3 0.013869 https://developers.google.com/ 4 0.013718 http://facebook.com/ 5 0.012229 http://youtube.com/ 6 0.011484 https://l.facebook.com/ 7 0.011484 https://developers.facebook.com/ 8 0.010891 https://blog.dicoding.com/ 9 0.010891 http://dicoding.com/","title":"PageRank"},{"location":"pendahuluan1/","text":"Web Mining 2019 / Weni Pratiwi S (160411100013) Penambangan dan pencarian web atau web mining merupakan proses ektraksi pola dari data-data pada suatu website. Terdiri dari 3 bagian yaitu : Web content mining Web content mining adalah proses ekstraksi pola/informasi dari dokumen atau data. Cara kerjanya adalah dengan cara mengekstraksi key word dari data, bisa berupa teks, citra, audio, video, metadata dan hyperlink. Web structure mining Web structure mining (log mining) digunakan untuk menemukan struktur link dari suatu website, yang nantinya akan membentuk suatu rangkuman dari bangunan website. Web usage mining Web usage mining adalah proses pengambilan informasi perilaku user dari log, click stream, query dan cookies. Untuk mendapatkan informasi dari web kita perlu melakukan crawl yaitu membaca semua halaman dan membuat index dari data yang dicari, dengam menggunakan Web Crawler . Web crawler merupakan serangkaian script pemrograman yang digunakan untuk melakukan crawl secara otomatis, dan menyimpan data yang diperoleh untuk selanjutnya diolah untuk mendapatkan informasi yang diinginkan. Requirement : Python BeatifulSoup4 Pandas SQLite3 Numpy Scipy Sastrawi Sckit-learn Sckit-fuzzy Networkx Matplotlib Math Database KBI import CSV Install requirement menggunakan pip di command prompt, atau download Anaconda. Library Anaconda sudah lengkap, anda bisa langsung menggunakan library yang anda butuhkan untuk bahasa python.","title":"Home"},{"location":"pendahuluan1/#requirement","text":"Python BeatifulSoup4 Pandas SQLite3 Numpy Scipy Sastrawi Sckit-learn Sckit-fuzzy Networkx Matplotlib Math Database KBI import CSV Install requirement menggunakan pip di command prompt, atau download Anaconda. Library Anaconda sudah lengkap, anda bisa langsung menggunakan library yang anda butuhkan untuk bahasa python.","title":"Requirement :"},{"location":"prepro/","text":"Pada tahap ini, data yang telah kita dapat perlu dilakukan pre-processing atau seleksi untuk mendapatkan key word yang akan menjadi sebuah fitur (kata penting). Tahapan preprocessing : Tokenisasi, proses pemecahan kata menjadi index-index tersendiri Stopword, proses seleksi membuang kata yang termasuk kata hubung dan tanda baca Stemming, proses pengambilan kata dasar dari setiap kata Seleksi berdasarkan KBI, kata dasar yang telah didapat dari porses stemming akan dicocokan pada kata yang terdapat pada database KBI untuk mendapatkan kata baku saja. Menambahkan kosa kata tidak penting pada list stopword dengan cara manual, yaitu mengakses Sastrawi\\StopWordRemover\\StopWordRemoverFactory.py dari library python anda Anaconda3\\Lib\\site-packages\\ . ekstraksi kata dasar : factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover () factorym = StemmerFactory () stemmer = factorym.create_stemmer () tmp = '' for i in deskripsi: tmp = tmp + ' ' +i hasil = [] for i in tmp.split(): try : if i.isalpha() and (not i in hasil) and len(i)>2: # Menghilangkan Kata tidak penting stop = stopword.remove(i) if stop != \"\": out = stemmer.stem(stop) hasil.append(out) except: continue katadasar=hasil Code diatas digunakan untuk mengekstaksi kata dasar, membuang fitur yang bukan berupa text dan fitur yang termasuk kata hubung menggunakan library dari sastrawi. contoh : input : \"saya jalan2 membeli sayur di pasar\" output : \"saya\" \"jalan\" \"beli\" \"sayur\" \"pasar\" cek kata dalam KBI : conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found Dalam proses ini kita memerlukan database KBI untuk mengecek setiap kata yang sesuai/baku. Kemudian buat sebuah function LinearSearch untuk mengecek setiap kata pada katadasar yang sesuai dengan KBI, dengan return value found . found bernilai true - false berhasil=[] for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) Jika found=true maka kata tersebut merupakan kata baku sesuai KBI, jadi kata tersebut kita simpan pada list berhasil . Jika found=false maka kata tersebut bukan kata baku sesuai KBI, jadi kata tersebut tidak perlu disimpan.","title":"Preprocessing"},{"location":"tfidf/","text":"TF-IDF atau Term Frequence dan Invers Document Frequence adalah proses pembobotan term pada setiap data yang heterogen. menghitung TF TF sama dengan VSM yaitu menghitung frekuensi kemunculan kata pada setiap data. tf = matrix menghitung IDF menghitung jumlah data secara luas pada koleksi dokumen yang bersangkutan. menghitung DF (Document Frekuensi) contoh : data 1 = \"telur ayam goreng\" data 2 = \"masak telur goreng makan ayam goreng\" kata jumlah data yang mengandung kata tersebut ayam 2 goreng 2 masak 1 makan 1 telur 2 df = list() for d in range (len(matrix[0])): total = 0 for i in range(len(matrix)): if matrix[i][d] !=0: total += 1 df.append(total) idf = list() for i in df: tmp = 1 + log10(len(matrix)/(1+i)) idf.append(tmp) list df digunakan untuk menampung hasil perhitungan DF, kemudian kita hitung IDF yang merupakan hasil inverse dari DF dan kita tampung pada list idf . tfidf = [] for baris in range(len(matrix)): tampungBaris = [] for kolom in range(len(matrix[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) Hitung TF-IDF, dengan mengalikan TF dengan IDF tf-idf = tf x idf , kemudian hasil tfidf disimpan pada list tfidf . with open('TFIDF.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(berhasil) for i in tfidf: employee_writer.writerow(i) tampung hasil TF-IDF pada file csv. TF-IDF digunakan untuk mendapatkan bobot yang lebih akurat dibandingkan dengan hasil VSM.","title":"TF-IDF"},{"location":"vsm/","text":"VSM merupakan proses menghitung frekuensi kemunculan setiap kata pada setiap data, dalam bentuk matriks. contoh : data 1 = \"telur ayam goreng\" data 2 = \"masak telur goreng makan ayam goreng\" data ke ayam goreng masak makan telur 1 1 1 0 0 1 2 1 2 1 1 1 conn = sqlite3.connect('events.db') matrix=[] cursor = conn.execute(\"SELECT* from EVENTS\") for row in cursor: tampung = [] for i in berhasil: tampung.append(row[2].lower().count(i)) matrix.append(tampung) code diatas akan mengecek dan menghitung berapa kali kata pada list berhasil muncul pada setiap baris data dalam tabel EVENTS di database events.db , kemudian menampung hasil frekuensi kemunculan .count() dalam list matrix . with open('VSMkbi.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(berhasil) for i in matrix: employee_writer.writerow(i) kemudian simpan hasil dalam file csv, dengan kolom pertama berisi kata/fitur dan kolom selanjutnya berisi matrix frekuensi kemunculan.","title":"VSM"}]}